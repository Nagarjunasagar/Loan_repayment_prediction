{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Loan Repayment Prediction_EDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_WxdhzBy2zk6LH1_lCz-Dpd1AATjgJvD",
      "authorship_tag": "ABX9TyPQwHFKYDbGNkbCQYqh8OdQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nagarjunasagar/Loan_repayment_prediction/blob/main/Loan_Repayment_Prediction_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loan Repayment Prediction**"
      ],
      "metadata": {
        "id": "GMC38fs124Tb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Introduction**\n",
        "Objective of this work is to predict the Loan repayment ability of the applicant based on the historical loan application data.\n",
        "\n",
        "## This is an standard superviced classification problem statement\n",
        "- Supervised : Training data which includes labelles is used to train the model to predict labels.\n",
        "- Clasiification : The label is a binary variable where, \n",
        "  - 0 Represents Repayment of Loan on time\n",
        "  - 1 Represents Having difficulty in repayment of loan\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B3RyjwSt8pf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Data**\n",
        "Data is from kaggle, rovided by 'Home Credit' a service dedicated to provided lines of credit (loans) to the unbanked population, as part of 'Home Credit Default Risk Competition'.\n",
        "Actual dataset on kaggle has 7 different sources of data and data in the form of .csv files. Here in this work I used only two files 'application_train.csv' and 'application_test.csv' source of these data is 'Home credit' itself(Some other data is from a Bureau, which is not included in this work). \n",
        "\n",
        "## Understanding data \n",
        "- Training Data: \n",
        " - 'application_train.csv' is the training data wich has information about each  loan application at 'Home Credit', Every loan has its own row and is defined by the feature `SK_ID_CURR`. \n",
        " - 'application_train.csv' comes with `TARGET` is a binary variable representing, 0 : Loan repaid and 1 : The loan was not paid.\n",
        "\n",
        "- Testing data :  \n",
        " - 'application_test.csv' is the testing data, has all the features/columns in training dataset except `TARGET`\n",
        "- More details about columns is provided in 'columns_description.xlsx' file\n",
        " \n"
      ],
      "metadata": {
        "id": "pwJ72XnhAiR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Libraries**\n",
        "We are using typical Datascience Stack `numpy`, `pandas`, `sklearn`, `matplotlib`, `seaborn`"
      ],
      "metadata": {
        "id": "bGTPAJVwRCFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy and pandas for data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# matplotlib and seaborn for plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# To suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# sklearn preprocessing for dealing with categorical variables\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import os\n"
      ],
      "metadata": {
        "id": "clafMsgpSBwy"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Read the data from datasets**\n",
        "- List all the files\n",
        "- Read train data\n",
        "- Read test data"
      ],
      "metadata": {
        "id": "HRA0fUcwBkm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List files available\n",
        "print(os.listdir(\"/content/drive/MyDrive/Loan_repayment_prediction\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNZA2uWwzMSK",
        "outputId": "be9e21ce-e7c6-4e71-b5fd-549caf0865dd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['application_test.csv', 'application_train.csv', 'columns_description.xlsx']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "app_train = pd.read_csv('/content/drive/MyDrive/Loan_repayment_prediction/application_train.csv')\n",
        "print('Training data shape: ', app_train.shape)\n",
        "app_train.head()"
      ],
      "metadata": {
        "id": "6PjWKQh6zt6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing data features\n",
        "app_test = pd.read_csv('/content/drive/MyDrive/Loan_repayment_prediction/application_test.csv')\n",
        "print('Testing data shape: ', app_test.shape)\n",
        "app_test.head()"
      ],
      "metadata": {
        "id": "4b-4aFXy0Y9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Training data has 307511 rows and 122 columns, \n",
        "- Testing data has 48744 rows and 121 columns\n",
        "- Testing test is considerably smaller without `TARGET` column\n"
      ],
      "metadata": {
        "id": "nE6v96PmD1XU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis\n",
        "- The goal of Exploratory Data Analysis(EDA) is to learn what our data can tell us. This is an open-ended process where we use statistics to make figures to find trends, patterns, outliers and relationships  within the data.\n",
        "- It generally starts with high level overview, then narrows to a specific areas as we find intriguing areas of data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RdknapZvGWvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Examining the `TARGET` column**\n",
        "`TARGET` column is a binary variable, which has 0 and 1 as its values,\n",
        "where: \n",
        "- 0 Represents Repayment of Loan on time\n",
        "- 1 Represents Having difficulty in repayment of loan"
      ],
      "metadata": {
        "id": "MQ1e2iEwQXU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app_train[\"TARGET\"].value_counts()"
      ],
      "metadata": {
        "id": "ZrJJM4KNSFwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_train[\"TARGET\"].astype(int).plot.hist()"
      ],
      "metadata": {
        "id": "obiWD-FVTV-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an imbalanced class problem where the total number of a class of data (positive) is far less than the total number of another class of data (negative). There are far more loans that were repaid on time than loans that were not repaid. "
      ],
      "metadata": {
        "id": "m3wCPZNQZjkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Examining the missing values in the data**\n",
        "Lets look at the columns that had missing values and number of missing values in them. Since we cant examine based on their values, lets convert in to Percentage, and list the top."
      ],
      "metadata": {
        "id": "49yRY1cZaCQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate missing values by column# Funct \n",
        "def missing_values_table(df):\n",
        "        # Total missing values\n",
        "        mis_val = df.isnull().sum()\n",
        "        \n",
        "        # Percentage of missing values\n",
        "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
        "        \n",
        "        # Make a table with the results\n",
        "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "        \n",
        "        # Rename the columns\n",
        "        mis_val_table_ren_columns = mis_val_table.rename(\n",
        "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "        \n",
        "        # Sort the table by percentage of missing descending\n",
        "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
        "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
        "        '% of Total Values', ascending=False).round(1)\n",
        "        \n",
        "        # Print some summary information\n",
        "        print (\"Dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
        "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
        "              \" columns that have missing values.\")\n",
        "        \n",
        "        # Return the dataframe with missing information\n",
        "        return mis_val_table_ren_columns"
      ],
      "metadata": {
        "id": "acGNjRmCUB5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values statistics\n",
        "missing_values = missing_values_table(app_train)\n",
        "missing_values.head(20)"
      ],
      "metadata": {
        "id": "MsRDrxeDcyTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Columns and their types**\n",
        "Since the number of columns/features are high lets examine the number of columns based on datatype. `int64` and `float64` are numeric variables and `object` columns cotains strings that are categorical features."
      ],
      "metadata": {
        "id": "-r1ZzS__N7XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app_train.dtypes.value_counts()"
      ],
      "metadata": {
        "id": "qM19eQCVKawp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now look at the number of unique entries in each of the `object `(categorical) columns"
      ],
      "metadata": {
        "id": "jii-oo74QL2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of unique classes in each object column\n",
        "app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"
      ],
      "metadata": {
        "id": "YR35GTW3PpQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most of the categorical variables has small number of unique entries\n",
        "- We need to find a way to deal with these categorical values\n",
        "- Because Machine learning models cant learn from the text data in this case"
      ],
      "metadata": {
        "id": "U4xYlDmblyL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Encoding Categorical variables**\n",
        "Before we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as LightGBM). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process:\n",
        "1. **Label Encoding** : \n",
        "- Label Encoding is good for variables with 2 unique categories.\n",
        "- Assigns each unique category in a categorical variable with an integer. No new columns are created.\n",
        "- For label encoding, we use the Scikit-Learn `LabelEncoder`\n",
        "\n",
        "- The problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category.\n",
        "\n",
        "2. **One-hot Encoding** : \n",
        "- One-hot encoding is safe option for categorical variables with many classes, because it does not impose arbitrary values to categories.\n",
        "- Create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns.\n",
        "-  For one-hot encoding we use the pandas `get_dummies(df)` function.\n",
        "\n",
        "- The only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by PCA or other dimensionality reduction methods to reduce the number of dimensions (while still trying to preserve information).\n",
        "\n"
      ],
      "metadata": {
        "id": "m45TALAU1FfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Label encoding for variables/columns having 2 or less unique categories"
      ],
      "metadata": {
        "id": "iqxXsQHSPIDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le_count = 0\n",
        "\n",
        "# Iterate through the columns\n",
        "for col in app_train:\n",
        "  if app_train[col].dtype == 'object':\n",
        "    # for 2 or less unique categories\n",
        "    if len(list(app_train[col].unique())) <= 2:\n",
        "      # Train on training data\n",
        "      le.fit(app_train[col])\n",
        "      # Transeform both trainin and testing data\n",
        "      app_train[col] = le.transform(app_train[col])\n",
        "      app_test[col] = le.transform(app_test[col])\n",
        "\n",
        "      #keep track of how many columns were label encoded\n",
        "      le_count += 1\n",
        "\n",
        "print(\"%d Colums were label encoded\" % le_count)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vBfc1RTSQRnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using One-hot encoding for columns/variables having more than 2 unique categories "
      ],
      "metadata": {
        "id": "FoP2F5-HV55I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app_train = pd.get_dummies(app_train)\n",
        "app_test = pd.get_dummies(app_test)\n",
        "\n",
        "print(\"Training features shape :\", app_train.shape)\n",
        "print(\"Testing features shape :\",app_test.shape)"
      ],
      "metadata": {
        "id": "EWC51IeYQviy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Alligning Training and Tesing data**"
      ],
      "metadata": {
        "id": "ejJAROf2Xq-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There must same number of features/columns in both Training and Testing data but One-hot encoding create more columns in Training data because there were some categorical variables with categories not represented in the testing data.\n",
        "To remove the columns in Training data that are not present in testing data, Dtaframes must be `align` \n",
        "- First TARGET column must be extracted fro training data because that is not the part of testing data but we need it for future.\n",
        "- When we do align we must set `axis=1 `so that Dataframes must align based on columns not based on rows"
      ],
      "metadata": {
        "id": "lZHzqOW7X8Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = app_train['TARGET']\n",
        "\n",
        "#Align Training and Testing data keep the columns present in both dataframes\n",
        "app_train, app_test = app_train.align(app_test, join = 'inner', axis=1)\n",
        "\n",
        "#Add the target back in Training data\n",
        "app_train[\"TARGET\"] = train_labels\n",
        "\n",
        "print(\"Trainin Features Shape :\", app_train.shape)\n",
        "print(\"Testing features Shape : \", app_test.shape)"
      ],
      "metadata": {
        "id": "VyHNDmVrROoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now both Training and Testing data has same number of column so that can be fed to Machine Learning model\n",
        "- The number of features has grown significantly due to One-hot encoding\n",
        "- At some point we probably want to try dimensionality reduction (removing features that are not relevant) to reduce the size of the datasets."
      ],
      "metadata": {
        "id": "0FebXA_1ddm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Anomalies/Outliers** \n",
        "**Exploratory Data Analysis Continued :**\n",
        "While doing EDA, finding Anomalies within the data is important, anomalies in the data are,\n",
        "- May be due to miss-typed numbers\n",
        "- Errors in measuring equipment\n",
        "- Valid extreme measurements\n",
        "- One way to support anomalies quantitatively is by looking at the statistics of a column using the `describe` method."
      ],
      "metadata": {
        "id": "YXM4YVDQVedE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app_train['DAYS_BIRTH'].describe()"
      ],
      "metadata": {
        "id": "JWO_NbZRdQFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numbers in the `DAYS_BIRTH` column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:"
      ],
      "metadata": {
        "id": "TcdMadfC1uLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(app_train['DAYS_BIRTH']/-365) .describe()"
      ],
      "metadata": {
        "id": "bQdo-Mtg1O9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After converting `DAYS_BIRTH` to years, ages looks good. There were no outliers detected either on higher or lower end\n",
        "\n",
        "Now lets examine `DAYS_EMPLOYED` that is days employed."
      ],
      "metadata": {
        "id": "XeaKmSY52yR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(app_train['DAYS_EMPLOYED']/365).describe()"
      ],
      "metadata": {
        "id": "6BkQmsYn2PGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "max = 1000 YEARS..!!!! Looks unusal right.. Thats an outlier\n",
        "\n",
        "Examining `DAYS_EMPLOYED` shows that it has outliers."
      ],
      "metadata": {
        "id": "arcDVWN5kJNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employed');\n",
        "plt.xlabel(\"Days Employed\")\n"
      ],
      "metadata": {
        "id": "InfgUivLj4ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets look for the number of outliers we had inthe column\n",
        "anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n",
        "# other values\n",
        "non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n",
        "\n",
        "print(\"The number of outliers with employed days = 365243, are :\", len(anom))\n",
        "print(\"The non_anomalies default on %0.2f%% of loans\"  % (100 * non_anom['TARGET'].mean()))\n",
        "print(\"The anomalies default on %0.2f%% of loans\"  % (100 * anom['TARGET'].mean()))\n"
      ],
      "metadata": {
        "id": "V94mt6jp0rP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 55374 outlier entries with same value\n",
        "Well that is extremely interesting! It turns out that the anomalies have a lower rate of default.\n",
        "\n",
        "Handling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous."
      ],
      "metadata": {
        "id": "nxoZSpu6EwDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create anomalous flag column\n",
        "app_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED'] == 365243"
      ],
      "metadata": {
        "id": "vbKQ27W2bJ-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert anomalous values to nan values\n",
        "app_train['DAYS_EMPLOYED'].replace({365243 : np.nan}, inplace = True)\n",
        "app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employed anom')"
      ],
      "metadata": {
        "id": "3O-QfA4mb5_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution looks to be much more in line with what we would expect, and we also have created a new column to tell the model that these values were originally anomalous (becuase we will have to fill in the nans with some value, probably the median of the column). The other columns with DAYS in the dataframe look to be about what we expect with no obvious outliers.\n",
        "\n",
        "**As an extremely important note, anything we do to the training data we also have to do to the testing data. Let's make sure to create the new column and fill in the existing column with np.nan in the testing data.**"
      ],
      "metadata": {
        "id": "9UctdpLjghYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets count the number of outliers with value = 365243 in the 'DAYS_EMPLOYED ' column of test dataframe\n",
        "test_anom = app_test[app_test['DAYS_EMPLOYED'] == 365243] "
      ],
      "metadata": {
        "id": "Bk2sFP9jg2BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_anom)"
      ],
      "metadata": {
        "id": "P5ZrKX-1h0oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "there are 9274 entries in the column with value 365243 "
      ],
      "metadata": {
        "id": "AIcvZvGniD7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets make a new column from this column to store anomalous entries, because in the actual column we need to replace the anomalous values with 'nan' values"
      ],
      "metadata": {
        "id": "DjC7iCxvigOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating anew column\n",
        "app_test['TEST_ANOM_DAYS_EMPLOYED'] = app_test['DAYS_EMPLOYED'] == 365243\n",
        "\n",
        "# Replace the anomalous values in the app_test['DAYS_EMPLOYED'] column to nan values\n",
        "app_test['DAYS_EMPLOYED'].replace({365243 : np.nan}, inplace = True)\n",
        "app_test['DAYS_EMPLOYED'].plot.hist(title = 'app_test DAYS_EMPLOYED')"
      ],
      "metadata": {
        "id": "5_qN1nLzh4bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirming the number of columns in both train and test set after all the above modifications\n",
        "print(app_train.shape)\n",
        "print(app_test.shape)"
      ],
      "metadata": {
        "id": "AgC6N8d9kA56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Correlation**"
      ],
      "metadata": {
        "id": "iMm_uSsIl0j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the `target` using the `.corr` dataframe method.\n",
        "\n",
        "The correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:\n",
        "\n",
        "- .00 - .19 “very weak”\n",
        "- .20 - .39 “weak”\n",
        "- .40 - .59 “moderate”\n",
        "- .60 - .79 “strong”\n",
        "- .80 - 1.0 “very strong”"
      ],
      "metadata": {
        "id": "Li15M53Pmdvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the correlation with the Target and Sort\n",
        "correlations =app_train.corr()['TARGET'].sort_values()"
      ],
      "metadata": {
        "id": "5ECcL9SQlat-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print correlations\n",
        "print(\"Most positive Correlations are:\\n\", correlations.tail(15))\n",
        "print(\"\\nMost negetive Correlations are:\", correlations.head(15))"
      ],
      "metadata": {
        "id": "pI5vw5W_nzJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at some of more significant correlations: \n",
        "- The `DAYS_BIRTH` is the most positive correlation. (except for TARGET because the correlation of a variable with itself is always 1!) Looking at the documentation, DAYS_BIRTH is the age in days of the client at the time of the loan in negative days (for whatever reason!). \n",
        "- The correlation is positive, but the value of this feature is actually negative, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). \n",
        "- That's a little confusing, so we will take the absolute value of the feature and then the correlation will be negative."
      ],
      "metadata": {
        "id": "GQ2tmWKyx7fX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Effect of Age on  Repayment"
      ],
      "metadata": {
        "id": "vk-0_pLnyV7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the correlation of the positive days since birth and target\n",
        "app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\n",
        "app_train['DAYS_BIRTH'].corr(app_train['TARGET'])"
      ],
      "metadata": {
        "id": "7vEBXMwkyVVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often.\n",
        "\n",
        "Let's start looking at this variable. First, we can make a histogram of the age. We will put the x axis in years to make the plot a little more understandable."
      ],
      "metadata": {
        "id": "VvUw_e-uzOrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(app_train['DAYS_BIRTH']/365).plot.hist(title='Client Age in years')"
      ],
      "metadata": {
        "id": "7vq6cNmEx6UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the style of plots\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "# Plot the distribution of ages in Years\n",
        "plt.hist(app_train['DAYS_BIRTH']/365, edgecolor = 'k', bins = 25)\n",
        "plt.title('Age of Client')\n",
        "plt.xlabel('Age in Years')\n",
        "plt.ylabel('Count')"
      ],
      "metadata": {
        "id": "2aDlyMNioZ2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By itself, the distribution of age does not tell us much other than that there are no outliers as all the ages are reasonable. \n",
        "- To visualize the effect of the age on the target, we will next make a kernel density estimation plot (KDE) colored by the value of the target. \n",
        "- A kernel density estimate plot shows the distribution of a single variable and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). \n",
        "- We will use the seaborn kdeplot for this graph."
      ],
      "metadata": {
        "id": "oVTbHChw2DP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,8))\n",
        "\n",
        "#KDE plot of loans that repaid on time\n",
        "sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH']/365, label = 'TARGET == 0')\n",
        "\n",
        "#KDE plot of loans which were not repayed on time\n",
        "sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH']/365, label = 'TARGET == 1')\n",
        "\n",
        "# Labeling of plot\n",
        "plt.title('Distribution of Ages'); plt.xlabel('Age in Years'); plt.ylabel('Density')"
      ],
      "metadata": {
        "id": "yypw-O-I1oM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The target == 1 curve skews towards the younger end of the range. \n",
        "- Although this is not a significant correlation (-0.07 correlation coefficient)\n",
        "- This variable is likely going to be useful in a machine learning model because it does affect the target. \n",
        "\n",
        "\n",
        "- Let's look at this relationship in another way: average failure to repay loans by age bracket.\n",
        "- To make this graph, first we cut the age category into bins of 5 years each. \n",
        "- Then, for each bin, we calculate the average value of the target, \n",
        "- This tells us the ratio of loans that were not repaid in each age category."
      ],
      "metadata": {
        "id": "3za9ReU4xUPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Age information into a separate dataframe\n",
        "age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n",
        "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n",
        "\n",
        "# Bin the age data\n",
        "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n",
        "age_data.head(10)"
      ],
      "metadata": {
        "id": "RLTsRcSbq3xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by the bin and calculate averages\n",
        "age_groups  = age_data.groupby('YEARS_BINNED').mean()\n",
        "age_groups"
      ],
      "metadata": {
        "id": "09iyf_v1xhuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (8, 8))\n",
        "\n",
        "# Graph the age bins and the average of the target as a bar plot\n",
        "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
        "\n",
        "# Plot labeling\n",
        "plt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\n",
        "plt.title('Failure to Repay by Age Group');"
      ],
      "metadata": {
        "id": "DSOU1RtvxwHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There is a clear trend:**  \n",
        "- Younger applicants are more likely to not repay the loan! \n",
        "- The rate of failure to repay is above 10% for the youngest three age groups \n",
        "- Beolow 5% for the oldest age group.\n",
        "\n",
        "This is information that could be directly used by the bank:\n",
        "- Because younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This does not mean the bank should discriminate against younger clients, but it would be smart to take precautionary measures to help younger clients pay on time"
      ],
      "metadata": {
        "id": "Bl0aBpD1x8u-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exterior Sources\n",
        "\n",
        "- The 3 variables with the strongest negative correlations with the target are \n",
        " - EXT_SOURCE_1, \n",
        " - EXT_SOURCE_2, and \n",
        " - EXT_SOURCE_3. \n",
        "- According to the documentation, these features represent a \"normalized score from external data source\". \n",
        "- I'm not sure what this exactly means, \n",
        "- But it may be a cumulative sort of credit rating made using numerous sources of data.\n",
        "\n",
        "Let's take a look at these variables.\n",
        "\n",
        "First, we can show the correlations of the EXT_SOURCE features with the target and with each other."
      ],
      "metadata": {
        "id": "IjlK2iijyDt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the EXT_SOURCE variables and show correlations\n",
        "ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
        "ext_data_corrs = ext_data.corr()\n",
        "ext_data_corrs"
      ],
      "metadata": {
        "id": "5sYi8zJEx3pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Heatmap \n",
        "Correlation for External sources, TARGET and DAYS_BIRTH"
      ],
      "metadata": {
        "id": "FVqjiZV7I_O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (8, 6))\n",
        "\n",
        "# Heatmap of correlations\n",
        "sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
        "plt.title('Correlation Heatmap');"
      ],
      "metadata": {
        "id": "UZKbU5rkyIMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- All three EXT_SOURCE featureshave negative correlations with the target  \n",
        "- Indicating that as the value of the `EXT_SOURCE` increases, the client is more likely to repay the loan. \n",
        "- We can also see that `DAYS_BIRTH `is positively correlated with `EXT_SOURCE_1` indicating that maybe one of the factors in this score is the client age.\n",
        "\n"
      ],
      "metadata": {
        "id": "8izNikFyyTYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Effect of External sources on TARGET\n",
        "Next we can look at the distribution of each of these features colored by the value of the target. This will let us visualize the effect of this variable on the target."
      ],
      "metadata": {
        "id": "hYyAtIWyJNgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10, 12))\n",
        "\n",
        "# iterate through the sources\n",
        "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
        "    \n",
        "    # create a new subplot for each source\n",
        "    plt.subplot(3, 1, i + 1)\n",
        "    # plot repaid loans\n",
        "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n",
        "    # plot loans that were not repaid\n",
        "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n",
        "    \n",
        "    # Label the plots\n",
        "    plt.title('Distribution of %s by Target Value' % source)\n",
        "    plt.xlabel('%s' % source); plt.ylabel('Density');\n",
        "    \n",
        "plt.tight_layout(h_pad = 2.5)"
      ],
      "metadata": {
        "id": "y2ERE97SyP0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`EXT_SOURCE_3` displays the greatest difference between the values of the target. \n",
        "- We can clearly see that this feature has some relationship to the likelihood of an applicant to repay a loan. \n",
        "- The relationship is not very strong, in fact they are all considered very weak, but these variables will still be useful for a machine learning model to predict whether or not an applicant will repay a loan on time."
      ],
      "metadata": {
        "id": "hJ86A6M-yeMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pairs Plot :**\n",
        "\n",
        "- As a final exploratory plot, we can make a pairs plot of the `EXT_SOURCE` variables and the `DAYS_BIRTH` variable.\n",
        "- The Pairs Plot is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables.\n",
        "- Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with\n",
        "  - scatterplots on the upper triangle, \n",
        "  - histograms on the diagonal, and \n",
        "  - 2D kernel density plots and correlation coefficients on the lower triangle.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uo7CuQxvymrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the data for plotting\n",
        "plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n",
        "\n",
        "# Add in the age of the client in years\n",
        "plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n",
        "\n",
        "# Drop na values and limit to first 100000 rows\n",
        "plot_data = plot_data.dropna().loc[:100000, :]\n",
        "\n",
        "# Function to calculate correlation coefficient between two columns\n",
        "def corr_func(x, y, **kwargs):\n",
        "    r = np.corrcoef(x, y)[0][1]\n",
        "    ax = plt.gca()\n",
        "    ax.annotate(\"r = {:.2f}\".format(r),\n",
        "                xy=(.2, .8), xycoords=ax.transAxes,\n",
        "                size = 20)\n",
        "\n",
        "# Create the pairgrid object\n",
        "grid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n",
        "                    hue = 'TARGET', \n",
        "                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n",
        "\n",
        "# Upper is a scatter plot\n",
        "grid.map_upper(plt.scatter, alpha = 0.2)\n",
        "\n",
        "# Diagonal is a histogram\n",
        "grid.map_diag(sns.kdeplot)\n",
        "\n",
        "# Bottom is density plot\n",
        "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
        "\n",
        "plt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);"
      ],
      "metadata": {
        "id": "oZejSvm6yaUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this plot\n",
        "- The red indicates loans that were not repaid and the blue are loans that are paid.\n",
        "- We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the EXT_SOURCE_1 and the DAYS_BIRTH (or equivalently YEARS_BIRTH), \n",
        "- Indicating that this feature may take into account the age of the client."
      ],
      "metadata": {
        "id": "_sYBSUUEz694"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "-emwVGW3l4gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Applied machine learning is basically feature engineering.\" - Andrew Ng\n",
        "\n",
        "\n",
        "Feature engineering is important in solving any Machine Learning problem, the most useful features that we create out of data defines how successful we are in adressing the problem.\n",
        "\n",
        "Feature engineering has a greater return on investment than model building and hyperparameter tuning\n",
        "\n",
        "While choosing the right model and optimal settings are important, the model can only learn from the data it is given. Making sure this data is as relevant to the task as possible is the job of the data scientist.\n",
        "\n",
        "Feature engineering refers to a geneal process and can involve both \n",
        "- **Feature construction:** Adding new features from the existing data, \n",
        "- **Feature selection:** choosing only the most important features or other methods of dimensionality reduction. \n",
        "\n",
        "There are many techniques we can use to both create features and select features.\n",
        "\n",
        "We will do a lot of feature engineering when we start using the other data sources, but in this notebook we will try only two simple feature construction methods:\n",
        "\n",
        "- Polynomial features\n",
        "- Domain knowledge features"
      ],
      "metadata": {
        "id": "ZOfV44q9mCbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Polynomial Features**\n",
        "\n",
        "- One simple feature construction method is called polynomial features. In this method, we make features that are powers of existing features as well as interaction terms between existing features.\n",
        "- For example, we can create variables EXT_SOURCE_1^2 and EXT_SOURCE_2^2 and also variables such as EXT_SOURCE_1 x EXT_SOURCE_2, EXT_SOURCE_1 x EXT_SOURCE_2^2, EXT_SOURCE_1^2 x EXT_SOURCE_2^2, and so on.\n",
        "- These features that are a combination of multiple individual variables are called interaction terms because they capture the interactions between variables.\n",
        "- In other words, while two variables by themselves may not have a strong influence on the target, combining them together into a single interaction variable might show a relationship with the target.\n",
        "- Interaction terms are commonly used in statistical models to capture the effects of multiple variables, but I do not see them used as often in machine learning. Nonetheless, we can try out a few to see if they might help our model to predict whether or not a client will repay a loan."
      ],
      "metadata": {
        "id": "_AdRKEyDmS9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Polynomial Features \n",
        "- In the following code, we create polynomial features using the `EXT_SOURCE` variables and the `DAYS_BIRTH` variable.\n",
        "- **Scikit-Learn** has a useful class called `PolynomialFeatures` that creates the polynomials and the interaction terms up to a specified degree. \n",
        "- We can use a **degree of 3** to see the results (when we are creating polynomial features, we want to avoid using too high of a degree, both because the number of features scales exponentially with the degree, and because we can run into problems with overfitting)."
      ],
      "metadata": {
        "id": "_BcJDXllnF_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new dataframe for polynomial features\n",
        "poly_features_train = app_train[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH','TARGET']]\n",
        "poly_features_test = app_test[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]"
      ],
      "metadata": {
        "id": "mQLh9uAHy0Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "e1jBuZyyKjfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imputer for handling missing values\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy = 'median')\n",
        "\n",
        "poly_target = poly_features_train['TARGET']\n",
        "\n",
        "poly_features_train = poly_features_train.drop(columns = ['TARGET'])\n",
        "\n",
        "#Nedd to impute missing values\n",
        "poly_features_train = imputer.fit_transform(poly_features_train)\n",
        "poly_features_test = imputer.fit_transform(poly_features_test)\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Create a polynmial object with specified degree 3\n",
        "poly_transformer = PolynomialFeatures(degree=3)"
      ],
      "metadata": {
        "id": "xWxboGs1o77g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the polynomial features\n",
        "poly_transformer.fit(poly_features_train)\n",
        "\n",
        "# Transform the features\n",
        "poly_features_train = poly_transformer.transform(poly_features_train)\n",
        "poly_features_test = poly_transformer.transform(poly_features_test)"
      ],
      "metadata": {
        "id": "v_EfPBNus1ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Polynomials features Shape\", poly_features_train.shape)"
      ],
      "metadata": {
        "id": "7GH_TxiXOhoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates a considerable number of new features. To get the names we have to use the polynomial features get_feature_names method."
      ],
      "metadata": {
        "id": "meEDQXfGPNW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH'])[:15]"
      ],
      "metadata": {
        "id": "_1ERyqhxO86J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are 35 features with individual features raised to powers up to degree 3 and interaction terms. \n",
        "- Now, we can see whether any of these new features are correlated with the target."
      ],
      "metadata": {
        "id": "mmrl-VNVQon5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe of the features\n",
        "poly_features = pd.DataFrame(poly_features_train, columns = poly_transformer.get_feature_names(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n",
        "\n",
        "# Add in the target\n",
        "poly_features['TARGET'] = poly_target\n",
        "\n",
        "#find correlations with the target\n",
        "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
        "\n",
        "# Display most positive correlations\n",
        "print('most positive correlations are :\\n',poly_corrs.tail())\n",
        "\n",
        "# Display most negetive correlations\n",
        "print('\\nmost negetive correlations are :\\n',poly_corrs.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "9fg-pWlMP_RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Several of the new variables have a greater (in terms of absolute magnitude) correlation with the target than the original features. \n",
        "- When we build machine learning models, we can try with and without these features to determine if they actually help the model learn.\n",
        "\n",
        "- We will add these features to a copy of the training and testing data and then evaluate models with and without the features. \n",
        "- **Many times in machine learning, the only way to know if an approach will work is to try it out..!!!!!**"
      ],
      "metadata": {
        "id": "kbKWdyxkv-Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put test features in to dataframe\n",
        "poly_features_test = pd.DataFrame(poly_features_test, columns = poly_transformer.get_feature_names(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']))\n",
        "\n",
        "# Merge polynomial features in to Training dataframe\n",
        "poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n",
        "app_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left' )\n",
        "\n",
        "# Mege Polynomial features in to testing dataframe\n",
        "poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n",
        "app_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left' )\n",
        "\n",
        "\n",
        "# Align the Dataframes\n",
        "app_train_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
        "\n",
        "#Print the new shapes\n",
        "print('Shape of New training Dataframe\\n', app_train_poly.shape())\n",
        "print('Shape of New testing Dataframe\\n', app_test_poly.shape())"
      ],
      "metadata": {
        "id": "DtAuncUjkEHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AQK_DCBVyE4f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}